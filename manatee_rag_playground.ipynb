{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting global variables(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docx2txt in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (0.12.20)\n",
      "Requirement already satisfied: ollama in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.4.7)\n",
      "Requirement already satisfied: llama-hub in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.0.79.post1)\n",
      "Requirement already satisfied: llama-index-core in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (0.12.20)\n",
      "Requirement already satisfied: llama-index-llms-ollama in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (0.5.2)\n",
      "Requirement already satisfied: llama-index-vector-stores-qdrant in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (0.4.3)\n",
      "Requirement already satisfied: llama-index-embeddings-huggingface in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (0.5.2)\n",
      "Requirement already satisfied: qdrant-client in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (1.13.2)\n",
      "Requirement already satisfied: sentence-transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (3.4.1)\n",
      "Requirement already satisfied: EbookLib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (0.18)\n",
      "Requirement already satisfied: html2text in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (2024.2.26)\n",
      "Requirement already satisfied: arize-phoenix in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (8.6.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (2.32.3)\n",
      "Requirement already satisfied: pypdf in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (5.3.0)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 20)) (1.0.1)\n",
      "Requirement already satisfied: docx2txt in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (0.8)\n",
      "Requirement already satisfied: python-pptx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from -r requirements.txt (line 23)) (1.0.2)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 2)) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 2)) (0.6.8)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 2)) (0.3.22)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 2)) (0.4.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 2)) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 2)) (0.4.5)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 2)) (3.9.1)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ollama->-r requirements.txt (line 3)) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from ollama->-r requirements.txt (line 3)) (2.10.6)\n",
      "Requirement already satisfied: psutil in /Users/oliviabyun/Library/Python/3.11/lib/python/site-packages (from llama-hub->-r requirements.txt (line 4)) (5.9.5)\n",
      "Requirement already satisfied: pyaml<24.0.0,>=23.9.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-hub->-r requirements.txt (line 4)) (23.12.0)\n",
      "Requirement already satisfied: retrying in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-hub->-r requirements.txt (line 4)) (1.3.4)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core->-r requirements.txt (line 7)) (2.0.38)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (3.11.13)\n",
      "Requirement already satisfied: dataclasses-json in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (2025.2.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (3.4.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (11.1.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-core->-r requirements.txt (line 7)) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.60.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-vector-stores-qdrant->-r requirements.txt (line 10)) (1.70.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 11)) (0.25.2)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from qdrant-client->-r requirements.txt (line 13)) (1.70.0)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from qdrant-client->-r requirements.txt (line 13)) (2.10.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from qdrant-client->-r requirements.txt (line 13)) (2.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 14)) (4.48.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 14)) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 14)) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 14)) (1.15.2)\n",
      "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from EbookLib->-r requirements.txt (line 15)) (5.3.1)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from EbookLib->-r requirements.txt (line 15)) (1.17.0)\n",
      "Requirement already satisfied: aioitertools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (0.12.0)\n",
      "Requirement already satisfied: aiosqlite in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (0.21.0)\n",
      "Requirement already satisfied: alembic<2,>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (1.14.1)\n",
      "Requirement already satisfied: arize-phoenix-client in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (1.0.2)\n",
      "Requirement already satisfied: arize-phoenix-evals>=0.13.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (0.20.3)\n",
      "Requirement already satisfied: arize-phoenix-otel>=0.5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (0.8.0)\n",
      "Requirement already satisfied: authlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (1.5.0)\n",
      "Requirement already satisfied: cachetools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (5.5.0)\n",
      "Requirement already satisfied: fastapi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (0.115.0)\n",
      "Requirement already satisfied: grpc-interceptor in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (0.15.4)\n",
      "Requirement already satisfied: jinja2 in /Users/oliviabyun/Library/Python/3.11/lib/python/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (3.1.2)\n",
      "Requirement already satisfied: openinference-instrumentation>=0.1.12 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (0.1.23)\n",
      "Requirement already satisfied: openinference-semantic-conventions>=0.1.12 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (0.1.14)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-proto>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-sdk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (0.51b0)\n",
      "Requirement already satisfied: pandas>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (2.2.3)\n",
      "Requirement already satisfied: protobuf<6.0,>=3.20.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (5.29.3)\n",
      "Requirement already satisfied: pyarrow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (17.0.0)\n",
      "Requirement already satisfied: python-multipart in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (0.0.12)\n",
      "Requirement already satisfied: sqlean-py>=3.45.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (3.47.0)\n",
      "Requirement already satisfied: starlette in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (0.38.6)\n",
      "Requirement already satisfied: strawberry-graphql==0.253.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (0.253.1)\n",
      "Requirement already satisfied: uvicorn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (0.31.1)\n",
      "Requirement already satisfied: websockets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from arize-phoenix->-r requirements.txt (line 17)) (12.0)\n",
      "Requirement already satisfied: graphql-core<3.4.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from strawberry-graphql==0.253.1->arize-phoenix->-r requirements.txt (line 17)) (3.2.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from strawberry-graphql==0.253.1->arize-phoenix->-r requirements.txt (line 17)) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->-r requirements.txt (line 18)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->-r requirements.txt (line 18)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->-r requirements.txt (line 18)) (2025.1.31)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-pptx->-r requirements.txt (line 23)) (3.2.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 7)) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 7)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 7)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 7)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 7)) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 7)) (1.18.3)\n",
      "Requirement already satisfied: Mako in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from alembic<2,>=1.3.0->arize-phoenix->-r requirements.txt (line 17)) (1.3.9)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client->-r requirements.txt (line 13)) (65.5.0)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<0.29,>=0.27->ollama->-r requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<0.29,>=0.27->ollama->-r requirements.txt (line 3)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama->-r requirements.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client->-r requirements.txt (line 13)) (4.2.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 11)) (3.16.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 11)) (24.2)\n",
      "Requirement already satisfied: minijinja>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 11)) (2.7.0)\n",
      "Requirement already satisfied: openai>=1.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->-r requirements.txt (line 2)) (1.64.0)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->-r requirements.txt (line 2)) (0.1.13)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r requirements.txt (line 2)) (4.13.3)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r requirements.txt (line 2)) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk>3.8.1->llama-index->-r requirements.txt (line 2)) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk>3.8.1->llama-index->-r requirements.txt (line 2)) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk>3.8.1->llama-index->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: opentelemetry-api in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openinference-instrumentation>=0.1.12->arize-phoenix->-r requirements.txt (line 17)) (1.30.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=1.0->arize-phoenix->-r requirements.txt (line 17)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas>=1.0->arize-phoenix->-r requirements.txt (line 17)) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->ollama->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->ollama->-r requirements.txt (line 3)) (2.27.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core->-r requirements.txt (line 7)) (3.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 14)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 14)) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 14)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 14)) (0.5.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core->-r requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: cryptography in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from authlib->arize-phoenix->-r requirements.txt (line 17)) (44.0.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dataclasses-json->llama-index-core->-r requirements.txt (line 7)) (3.26.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/oliviabyun/Library/Python/3.11/lib/python/site-packages (from jinja2->arize-phoenix->-r requirements.txt (line 17)) (2.1.3)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.30.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from opentelemetry-exporter-otlp->arize-phoenix->-r requirements.txt (line 17)) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.30.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from opentelemetry-exporter-otlp->arize-phoenix->-r requirements.txt (line 17)) (1.30.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.30.0->opentelemetry-exporter-otlp->arize-phoenix->-r requirements.txt (line 17)) (1.65.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.30.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.30.0->opentelemetry-exporter-otlp->arize-phoenix->-r requirements.txt (line 17)) (1.30.0)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from opentelemetry-api->openinference-instrumentation>=0.1.12->arize-phoenix->-r requirements.txt (line 17)) (8.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 14)) (3.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio->httpx<0.29,>=0.27->ollama->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r requirements.txt (line 2)) (2.6)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client->-r requirements.txt (line 13)) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client->-r requirements.txt (line 13)) (4.1.0)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r requirements.txt (line 2)) (0.6.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index->-r requirements.txt (line 2)) (0.8.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/oliviabyun/Library/Python/3.11/lib/python/site-packages (from cryptography->authlib->arize-phoenix->-r requirements.txt (line 17)) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/oliviabyun/Library/Python/3.11/lib/python/site-packages (from cffi>=1.12->cryptography->authlib->arize-phoenix->-r requirements.txt (line 17)) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api->openinference-instrumentation>=0.1.12->arize-phoenix->-r requirements.txt (line 17)) (3.20.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n",
      "[nltk_data] Error loading punkt_tab: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import logging\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = os.getenv('API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "\n",
    "VECTOR_STORAGE_DIR = \"./vector\"\n",
    "DATA_STORAGE_DIR = \"./data\"\n",
    "\n",
    "os.makedirs(DATA_STORAGE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_2/(Articles) Class 5_6 - nouns, singular & plural - Beverly Reyes.pptx\n",
      "(Articles) Class 5_6 - nouns, singular & plural - Beverly Reyes.pptx\n",
      "./data_2/(Verbs Practice) Potencia Worksheet 1_Elaine - Shilpi Dey.docx\n",
      "(Verbs Practice) Potencia Worksheet 1_Elaine - Shilpi Dey.docx\n",
      "./data_2/(Verbs) 5 Nitieixan & Deinielle - Jakob Lattanzi.pptx\n",
      "(Verbs) 5 Nitieixan & Deinielle - Jakob Lattanzi.pptx\n",
      "./data_2/(Verbs) 2 To Be - Lara Creyghton.pptx\n",
      "(Verbs) 2 To Be - Lara Creyghton.pptx\n",
      "./data_2/(Verbs) English Class #3 - Danielle Coan.pptx\n",
      "(Verbs) English Class #3 - Danielle Coan.pptx\n",
      "./data_2/(Pronouns) Potencia - Lesson 3 - Nikita Goyal.docx\n",
      "(Pronouns) Potencia - Lesson 3 - Nikita Goyal.docx\n",
      "./data_2/(Pronouns_Prepositions) Class 6 - Amber Adelman.pptx\n",
      "(Pronouns_Prepositions) Class 6 - Amber Adelman.pptx\n",
      "./data_2/(Of_From_For) lesson #6 11.9.20 -- of_for_from - Moxie Thompson.png\n",
      "(Of_From_For) lesson #6 11.9.20 -- of_for_from - Moxie Thompson.png\n",
      "./data_2/(Verbs) - Paridhi Rathi.pptx\n",
      "(Verbs) - Paridhi Rathi.pptx\n",
      "./data_2/(Verbs) Lesson on irregular verbs - Dana.docx\n",
      "(Verbs) Lesson on irregular verbs - Dana.docx\n",
      "./data_2/(Misc Grammar) Potencia Class 2 - Camila Lamoratta.pdf\n",
      "(Misc Grammar) Potencia Class 2 - Camila Lamoratta.pdf\n",
      "./data_2/(Verbs_Citizenship Practice) Potencia Tutoring (1) - Nicole Page.docx\n",
      "(Verbs_Citizenship Practice) Potencia Tutoring (1) - Nicole Page.docx\n",
      "./data_2/(Verbs) 10 Lesson with Nitieixan - Jakob Lattanzi.pptx\n",
      "(Verbs) 10 Lesson with Nitieixan - Jakob Lattanzi.pptx\n",
      "./data_2/(Verb Tenses) Lesson #4   - Adriana Da Gama Henriques.pptx\n",
      "(Verb Tenses) Lesson #4   - Adriana Da Gama Henriques.pptx\n",
      "./data_2/(Verbs) English Class #2 - Danielle Coan.pptx\n",
      "(Verbs) English Class #2 - Danielle Coan.pptx\n",
      "./data_2/(Verbs) Lesson #3 - Adriana Da Gama Henriques.pptx\n",
      "(Verbs) Lesson #3 - Adriana Da Gama Henriques.pptx\n",
      "./data_2/(Vowels) lesson #7 11.16.20 -- long_short vowel sounds - Moxie Thompson.png\n",
      "(Vowels) lesson #7 11.16.20 -- long_short vowel sounds - Moxie Thompson.png\n",
      "./data_2/(Verbs) 16 Nitieixan & Deinielle - Jakob Lattanzi.pptx\n",
      "(Verbs) 16 Nitieixan & Deinielle - Jakob Lattanzi.pptx\n",
      "./data_2/(Verbs) Potencia Tutoring - Nicole Page.docx\n",
      "(Verbs) Potencia Tutoring - Nicole Page.docx\n",
      "./data_2/(Occupations Verb) lesson #6 11.9.20 -- ppl based on action_profession - Moxie Thompson.png\n",
      "(Occupations Verb) lesson #6 11.9.20 -- ppl based on action_profession - Moxie Thompson.png\n",
      "./data_2/(Pronouns_Food) Class 5  - Amber Adelman.pdf\n",
      "(Pronouns_Food) Class 5  - Amber Adelman.pdf\n",
      "./data_2/(Verbs) 2_to_be_by Sarah L.pptx.pptx\n",
      "(Verbs) 2_to_be_by Sarah L.pptx.pptx\n",
      "./data_2/(Prepositions_Verbs) Lesson 2 - Leila - Ashley Cornwell.pptx\n",
      "(Prepositions_Verbs) Lesson 2 - Leila - Ashley Cornwell.pptx\n",
      "./data_2/(Prepositions) - Paridhi Rathi.pptx\n",
      "(Prepositions) - Paridhi Rathi.pptx\n",
      "./data_2/(Verbs_Basics) 23 Lesson Nitieixan & Deinielle - Jakob Lattanzi.pptx\n",
      "(Verbs_Basics) 23 Lesson Nitieixan & Deinielle - Jakob Lattanzi.pptx\n",
      "./data_2/(Verbs) lesson #6 11.9.20 -- past simple of irregular verbs  - Moxie Thompson.png\n",
      "(Verbs) lesson #6 11.9.20 -- past simple of irregular verbs  - Moxie Thompson.png\n",
      "./data_2/(Verbs) 1 Nitieixan & Deinielle - Jakob Lattanzi.pptx\n",
      "(Verbs) 1 Nitieixan & Deinielle - Jakob Lattanzi.pptx\n",
      "./data_2/(Irregular Verbs) lesson #7 11.16.20 -- past simple of regular_irregular verbs - Moxie Thompson.png\n",
      "(Irregular Verbs) lesson #7 11.16.20 -- past simple of regular_irregular verbs - Moxie Thompson.png\n",
      "./data_2/(Verbs_Prepositions) Class 7 - Amber Adelman.pdf\n",
      "(Verbs_Prepositions) Class 7 - Amber Adelman.pdf\n",
      "./data_2/Tutor training handbook.pdf\n",
      "Tutor training handbook.pdf\n",
      "./data_2/(Grammar_ Nouns) Class 7 – possessive nouns, pronouns - Beverly Reyes.pptx\n",
      "(Grammar_ Nouns) Class 7 – possessive nouns, pronouns - Beverly Reyes.pptx\n",
      "./data_2/(Preposition_Possession) Lesson #2 - Adriana Da Gama Henriques.pptx\n",
      "(Preposition_Possession) Lesson #2 - Adriana Da Gama Henriques.pptx\n",
      "./data_2/(Prepositions_Verbs) Lesson 3 - Leila - Ashley Cornwell.pptx\n",
      "(Prepositions_Verbs) Lesson 3 - Leila - Ashley Cornwell.pptx\n",
      "./data_2/(Verb Tenses) - Paridhi Rathi.pptx\n",
      "(Verb Tenses) - Paridhi Rathi.pptx\n",
      "./data_2/(Grammar_Chores) Lesson 10- Linara.pdf\n",
      "(Grammar_Chores) Lesson 10- Linara.pdf\n",
      "./data_2/(Comparisons_Grammar) Lesson 17- Linara(1).pdf\n",
      "(Comparisons_Grammar) Lesson 17- Linara(1).pdf\n"
     ]
    }
   ],
   "source": [
    "def get_file_names_in_directory(directory_path):\n",
    "    \"\"\"List all files in the directory\"\"\"\n",
    "    file_paths = []\n",
    "    names = []\n",
    "    for dirpath, _, filenames in os.walk(directory_path):\n",
    "        for filename in filenames:\n",
    "            file_paths.append(os.path.join(dirpath, filename))\n",
    "            names.append(filename)\n",
    "    return file_paths, names\n",
    "\n",
    "# Get file paths and file names\n",
    "file_paths, file_names = get_file_names_in_directory(DATA_STORAGE_DIR)\n",
    "for file_path, file_name in zip(file_paths, file_names):\n",
    "    print(file_path)\n",
    "    print(file_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for datapreprocessing\n",
    "- load_any_documents\n",
    "- load_pptx_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.25.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "import fitz\n",
    "def load_pptx_text(file_path):\n",
    "    \"\"\"Given a pptx file path, extract all the text from it\"\"\"\n",
    "    pres = Presentation(file_path)\n",
    "    pptx_text = []\n",
    "    for slide in pres.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if shape.has_text_frame:\n",
    "                text_frame = shape.text_frame\n",
    "                for paragraph in text_frame.paragraphs:\n",
    "                    for run in paragraph.runs:\n",
    "                        pptx_text.append(run.text)\n",
    "                        \n",
    "    return pptx_text\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"Extract text from a PDF file using PyMuPDF.\"\"\"\n",
    "    text = []\n",
    "    try:\n",
    "        pdf_document = fitz.open(file_path)\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document[page_num]\n",
    "            text.append(page.get_text())\n",
    "        pdf_document.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file {file_path}: {e}\")\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def load_any_documents(dirpath:str):\n",
    "    \"\"\"Given a directory path, load all the files in it: .pptx, .docx, .txt, .pdf, png\"\"\"\n",
    "    documents = []\n",
    "    for file_name in os.listdir(dirpath):\n",
    "        file_path = os.path.join(dirpath, file_name)\n",
    "        \n",
    "        if file_name.endswith(\".pptx\"):\n",
    "            # handle pptx file\n",
    "            pptx_text = load_pptx_text(file_path)\n",
    "            documents.append(Document(text=\"\\n\".join(pptx_text), doc_id=file_name))\n",
    "        elif file_name.endswith(\".pdf\"):\n",
    "            pdf_text = extract_text_from_pdf(file_path)\n",
    "            documents.append(Document(text=pdf_text, doc_id=file_name))\n",
    "        else:\n",
    "            documents += (SimpleDirectoryReader(input_files=[file_path]).load_data())\n",
    "            \n",
    "    return documents\n",
    "\n",
    "\n",
    "# documents = load_any_documents(\"./data/Verbs\")\n",
    "# for doc in documents:\n",
    "#     print(f\"{type(doc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data \n",
    "- read from desginated directory using SimpleDirectoryReader, returns a list of document objects. \n",
    "- docx, ppt, pdf file formats are all supported\n",
    "\n",
    "## Storage Context\n",
    "- represents the location for both **storing** and **loading** the index data\n",
    "- In this demo, each subdirectory is represented as an index\n",
    "  - We could explore other alternatives. \n",
    "\n",
    "```\n",
    "./vector/Verbs/\n",
    "├── Category1/\n",
    "│   ├── SubcategoryA/\n",
    "│   │   ├── file1.txt\n",
    "│   │   └── file2.txt\n",
    "│   └── SubcategoryB/\n",
    "│       └── file3.txt\n",
    "└── Category2/\n",
    "    └── file4.txt\n",
    "\n",
    "./data/Verbs/\n",
    "├── Category1/\n",
    "│   ├── SubcategoryA/       # Indexes for SubcategoryA files are stored here\n",
    "│   └── SubcategoryB/       # Indexes for SubcategoryB files are stored here\n",
    "└── Category2/              # Indexes for Category2 files are stored here\n",
    "```\n",
    "\n",
    "## Indexing\n",
    "- generates embedding for each document, using designated embedding model. \n",
    "- VectorStoreIndex.from_documents: generate index from document nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 10:29:53,346 [INFO] Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# initialize an index list\n",
    "index_list = []\n",
    "# description_list = []\n",
    "## represent index represent a subdirectory\n",
    "\n",
    "os.makedirs(VECTOR_STORAGE_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=VECTOR_STORAGE_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "    index_list.append(index)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logger.info(f\"Index for main directory not found. Creating a new index.\")\n",
    "\n",
    "    documents = load_any_documents(DATA_STORAGE_DIR)\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    index.storage_context.persist(VECTOR_STORAGE_DIR)\n",
    "    index_list.append(index)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error processing main directory: {e}\")    \n",
    "\n",
    "# for dirpath, _, file_names in os.walk(DATA_STORAGE_DIR):\n",
    "#     if file_names:\n",
    "#         relative_path = os.path.relpath(dirpath, DATA_STORAGE_DIR)\n",
    "#         storage_dir = os.path.join(VECTOR_STORAGE_DIR, relative_path)\n",
    "#         os.makedirs(storage_dir, exist_ok=True)\n",
    "        \n",
    "#         try:\n",
    "#             storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
    "#             index = load_index_from_storage(storage_context)\n",
    "#             description = storage_dir.split('/')[-1]\n",
    "#             print(description)\n",
    "#             index_list.append(index)\n",
    "#             description_list.append(description)\n",
    "#             logger.info(f\"Loaded existing index for subdirectory {relative_path}.\")\n",
    "#         except FileNotFoundError:\n",
    "#             logger.info(f\"Index for {relative_path} not found. Creating a new index.\")\n",
    "            \n",
    "#             # load all document in current subdirectory\n",
    "#             documents = load_any_documents(dirpath)\n",
    "#             # documents = [{\"id\": doc[\"id\"], \"content\": doc[\"content\"]} for doc in documents]\n",
    "#             index = VectorStoreIndex.from_documents(documents)\n",
    "#             index.storage_context.persist(storage_dir)\n",
    "#             # index = \"Dummy\"\n",
    "#             # add each index to index list\n",
    "#             index_list.append(index)\n",
    "            \n",
    "            \n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Error processing subdirectory {relative_path}: {e}\")     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Engine\n",
    "- in the previous code block, we converted all the files in Verbs example directory to index(Embeddings)\n",
    "- Now we can retrieve relevant block using the query engine. \n",
    "- A query engine in the context of information retrieval and language models is a component that processes queries by searching through a collection of data (like a set of documents or a database) and returning the most relevant information. \n",
    "\n",
    "### Retriever\n",
    "- Retrievers are responsible for fetching the most relevant context given a user query (or chat message).\n",
    "\n",
    "### RouterQueryEngine\n",
    "- Routers are modules that take in a user query and a set of \"choices\" (defined by metadata), and returns one or more selected choices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'description_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m chat_engines \u001b[39m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m query_engine_tools \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfor\u001b[39;00m description, index \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(description_list, index_list):\n\u001b[1;32m     25\u001b[0m     query_engine \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39mas_query_engine(similarity_top_k\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m     26\u001b[0m     query_engine_tools\u001b[39m.\u001b[39mappend(\n\u001b[1;32m     27\u001b[0m         QueryEngineTool\u001b[39m.\u001b[39mfrom_defaults(query_engine\u001b[39m=\u001b[39mquery_engine, \n\u001b[1;32m     28\u001b[0m                                       description\u001b[39m=\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mused to handle queries related to \u001b[39m\u001b[39m{\u001b[39;00mdescription\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'description_list' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector\n",
    "from llama_index.core.selectors import (\n",
    "    PydanticMultiSelector,\n",
    "    PydanticSingleSelector,\n",
    ")\n",
    "def pretty_print_nodes_with_scores(nodes):\n",
    "    \n",
    "    for node_with_score in nodes:\n",
    "        node = node_with_score.node\n",
    "        score = node_with_score.score\n",
    "    \n",
    "        # Print relevant content (text) and metadata\n",
    "        print(\"Content:\", node.text)\n",
    "        print(\"Relevance Score:\", score)\n",
    "        print(\"Source:\", node.metadata.get(\"source\"))\n",
    "        print(\"Page:\", node.metadata.get(\"page\", \"N/A\"))\n",
    "        print(\"\\n---\\n\")\n",
    "    \n",
    "# 1. query_engine\n",
    "# Most likely, we would have multiple query engines\n",
    "chat_engines = []\n",
    "query_engine_tools = []\n",
    "for description, index in zip(description_list, index_list):\n",
    "    query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "    query_engine_tools.append(\n",
    "        QueryEngineTool.from_defaults(query_engine=query_engine, \n",
    "                                      description=(f\"used to handle queries related to {description}.\")))\n",
    "    chat_engines.append(index.as_chat_engine())\n",
    "\n",
    "# response = query_engine[2].query(\"How much does th tutor gets paid?\")\n",
    "# response = chat_engines[0].chat(\"How much does the tutor gets paid?\")\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=PydanticSingleSelector.from_defaults(),\n",
    "    query_engine_tools=query_engine_tools\n",
    ")\n",
    "# response = query_engine.query(\"What tools are recommended for virtual meetings?\")\n",
    "# response = query_engine.query(\"I am going to give you a query to generate a lesson plan. Here are some general guidelines, but if the query says something contradicting this, go with the query. The lessons are taught in only English. Make sure that every new topic has a question posed alongside it. Questions are insightful and appropriate. Make sure the lesson has appropriate icebreakers at the beginning and asks students about their day, lessons maintain a friendly atmosphere. Generate 3 or more specific sets of practice problems. Create a brief introduction to the topic. The lesson plan should contain 5 or more specific examples. The lesson plan should have the following sections: Objective, Warm-up, Introduction, Explanation, Examples, Practice Problems, Interactive Activity, and Conclusion. Based on these guidelines, generate a lesson plan given the following topic: Prepositions for a beginner English learner.\")\n",
    "# response = chat_engines[0].query(\"I am going to give you a query to generate a lesson plan. Here are some general guidelines, but if the query says something contradicting this, go with the query. The lessons are taught in only English. Make sure that every new topic has a question posed alongside it. Questions are insightful and appropriate. Make sure the lesson has appropriate icebreakers at the beginning and asks students about their day, lessons maintain a friendly atmosphere. Generate 3 or more specific sets of practice problems. Create a brief introduction to the topic. The lesson plan should contain 5 or more specific examples. The lesson plan should have the following sections: Objective, Warm-up, Introduction, Explanation, Examples, Practice Problems, Interactive Activity, and Conclusion. Based on these guidelines, generate a lesson plan given the following topic: Verb Past Tense for a beginner English learner.\")\n",
    "response = chat_engines[0].chat(\"I am going to give you a query to generate a lesson plan. Here are some general guidelines, but if the query says something contradicting this, go with the query. The lessons are taught in only English. Make sure that every new topic has a question posed alongside it. Questions are insightful and appropriate. Make sure the lesson has appropriate icebreakers at the beginning and asks students about their day, lessons maintain a friendly atmosphere. Generate 3 or more specific sets of practice problems. Create a brief introduction to the topic. The lesson plan should contain 5 or more specific examples. The lesson plan should have the following sections: Objective, Warm-up, Introduction, Explanation, Examples, Practice Problems, Interactive Activity, and Conclusion. Based on these guidelines, generate a lesson plan given the following topic: Verb Past Tense for a beginner English learner.\")\n",
    "print(str(response))\n",
    "\n",
    "# response2 = chat_engines[0].query(\"Following up on my last response, can you give more examples?\")\n",
    "response2 = chat_engines[0].chat(\"Following up on my last response, can you give 5 specific examples?\")\n",
    "print(str(response2))\n",
    "\n",
    "print(\"finished\")\n",
    "# print(response)\n",
    "\n",
    "# 2. retriever\n",
    "# retriever = index_list[0].as_retriever()\n",
    "# nodes = retriever.retrieve(\"Who contributed to the handbook?\")\n",
    "\n",
    "# pretty_print_nodes_with_scores(nodes[:3])\n",
    "\n",
    "# 3. routerQueryEngine, if more than on query Engines are created then we can try this out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.13.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from beautifulsoup4) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test cases for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    {\"Question\": \"How does Potencia facilitate communication between tutors and learners?\",\n",
    "     \"Answer\": \"Through scheduled meetings and shared platforms.\"},\n",
    "    {\"Question\": \"What tools are recommended for virtual meetings?\",\n",
    "     \"Answer\": \"Zoom, Google Meet, and consistent links.\"},\n",
    "    {\"Question\": \"How can tutors help learners overcome challenges with technology?\",\n",
    "     \"Answer\": \"Provide guidance and resources.\"},\n",
    "    {\"Question\": \"Why is consistency in meeting links important for online sessions?\",\n",
    "     \"Answer\": \"It reduces confusion and ensures reliability.\"},\n",
    "    {\"Question\": \"What questions should tutors ask themselves after each session for self-reflection?\",\n",
    "     \"Answer\": \"What went well, and what can I improve?\"},\n",
    "    {\"Question\": \"How can reflecting with learners after class improve teaching effectiveness?\",\n",
    "     \"Answer\": \"It provides feedback and builds rapport.\"},\n",
    "    {\"Question\": \"What factors should tutors consider when deciding on the next topic to teach?\",\n",
    "     \"Answer\": \"Learner goals and prior progress.\"},\n",
    "    {\"Question\": \"What is the recommended structure for a tutoring session?\",\n",
    "     \"Answer\": \"Warm-up, main activity, wrap-up.\"},\n",
    "    {\"Question\": \"How should a session be wrapped up effectively?\",\n",
    "     \"Answer\": \"Summarize and discuss next steps.\"},\n",
    "    {\"Question\": \"What is the tutoring session policy regarding session logging and cancellations?\",\n",
    "     \"Answer\": \"Log sessions and inform in advance about cancellations.\"},\n",
    "    {\"Question\": \"What informal assessments are recommended during a class session?\",\n",
    "     \"Answer\": \"Observation and on-the-spot questions.\"},\n",
    "    {\"Question\": \"How can quizzes and reading activities help assess a learner's knowledge?\",\n",
    "     \"Answer\": \"They evaluate comprehension and retention.\"},\n",
    "    {\"Question\": \"What forms of support does Potencia offer to tutors during the semester?\",\n",
    "     \"Answer\": \"Workshops, feedback sessions, and resources.\"}\n",
    "]\n",
    "\n",
    "# Create a filtered DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to a new CSV file\n",
    "output_file = \"Tutor_Questions_and_Answers.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does Potencia facilitate communication between tutors and learners?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:40:25,993 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:25,999 [INFO] Selecting query engine 1: The Tutor training handbook is likely to contain information on how Potencia facilitates communication between tutors and learners..\n",
      "2025-03-12 16:40:26,343 [INFO] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:27,847 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What tools are recommended for virtual meetings?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:40:28,800 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:28,806 [INFO] Selecting query engine 1: The Tutor training handbook may contain information on tools recommended for virtual meetings..\n",
      "2025-03-12 16:40:29,415 [INFO] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:29,980 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can tutors help learners overcome challenges with technology?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:40:30,541 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:30,545 [INFO] Selecting query engine 1: The Tutor training handbook may provide guidance on how tutors can help learners overcome challenges with technology..\n",
      "2025-03-12 16:40:30,725 [INFO] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:31,567 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is consistency in meeting links important for online sessions?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:40:32,559 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:32,564 [INFO] Selecting query engine 1: The Tutor training handbook may provide guidelines and best practices for maintaining consistency in meeting links for online sessions..\n",
      "2025-03-12 16:40:32,862 [INFO] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:33,804 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What questions should tutors ask themselves after each session for self-reflection?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:40:34,309 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:34,315 [INFO] Selecting query engine 1: The question is about self-reflection after a session, which is related to tutor training handbook..\n",
      "2025-03-12 16:40:34,841 [INFO] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:35,765 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can reflecting with learners after class improve teaching effectiveness?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:40:36,263 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:36,268 [INFO] Selecting query engine 1: Reflecting with learners after class can be an important aspect of tutor training to improve teaching effectiveness..\n",
      "2025-03-12 16:40:36,411 [INFO] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:37,391 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What factors should tutors consider when deciding on the next topic to teach?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:40:38,428 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:38,437 [INFO] Selecting query engine 1: The Tutor training handbook may provide guidelines and factors for tutors to consider when deciding on the next topic to teach..\n",
      "2025-03-12 16:40:38,685 [INFO] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:39,392 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the recommended structure for a tutoring session?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:40:40,435 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:40,472 [INFO] Selecting query engine 1: The Tutor training handbook would likely contain information on the recommended structure for a tutoring session..\n",
      "2025-03-12 16:40:40,656 [INFO] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:41,702 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How should a session be wrapped up effectively?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:40:42,318 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:42,323 [INFO] Selecting query engine 1: The Tutor training handbook may provide guidelines on how to effectively wrap up a session with students..\n",
      "2025-03-12 16:40:42,624 [INFO] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:43,787 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the tutoring session policy regarding session logging and cancellations?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:40:44,624 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:44,634 [INFO] Selecting query engine 1: The Tutor training handbook is likely to contain policies and guidelines related to tutoring sessions, including session logging and cancellations..\n",
      "2025-03-12 16:40:44,884 [INFO] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:45,798 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What informal assessments are recommended during a class session?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:40:46,312 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:46,316 [INFO] Selecting query engine 1: The Tutor training handbook may provide information on recommended informal assessments during a class session..\n",
      "2025-03-12 16:40:46,515 [INFO] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:46,969 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can quizzes and reading activities help assess a learner's knowledge?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:40:47,615 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:47,619 [INFO] Selecting query engine 0: Quizzes and reading activities are often related to grammar, and the choice related to Misc Grammar would be the most relevant for assessing a learner's knowledge through quizzes and reading activities..\n",
      "2025-03-12 16:40:47,949 [INFO] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:49,118 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What forms of support does Potencia offer to tutors during the semester?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 16:40:49,712 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:49,732 [INFO] Selecting query engine 1: The query is related to tutor training handbook, which is likely to contain information about the forms of support Potencia offers to tutors during the semester..\n",
      "2025-03-12 16:40:49,911 [INFO] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-12 16:40:51,933 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Tutor_Questions_and_Answers.csv\")\n",
    "questions = df['Question']\n",
    "rag_responses = []\n",
    "for question in questions:\n",
    "    print(question)\n",
    "    rag_responses.append(query_engine.query(question))\n",
    "    \n",
    "df['RAG_Answer'] = rag_responses\n",
    "df.to_csv(output_file, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatEngine seems to perform much better than routerQuery engine\n",
    "Chat engine response: \n",
    "\n",
    "Here is a full lesson plan on verbs with specific practice examples:\n",
    "\n",
    "1. Introduction to Irregular Verb Tenses:\n",
    "   - Provide examples of irregular verbs in present and past tenses, such as \"am/was,\" \"write/wrote,\" \"draw/drew,\" \"do/did,\" \"make/made,\" \"meet/met,\" \"pay/paid,\" \"send/sent,\" \"sleep/slept,\" \"stand/stood,\" \"read/read,\" \"cut/cut,\" \"buy/bought,\" and \"see/saw.\"\n",
    "   \n",
    "2. Practice Activity:\n",
    "   - Engage students in creating sentences using both the present and past forms of irregular verbs. For example, transform \"I pay my bills every month\" into \"I paid the bills last month.\"\n",
    "   \n",
    "3. Future Tense:\n",
    "   - Introduce future tense examples like \"I am going to/I will\" for \"am/was,\" \"I will write\" for \"write/wrote,\" and so on. Have students practice creating sentences using the future tense forms of irregular verbs.\n",
    "   \n",
    "4. Conclusion and Homework:\n",
    "   - Review the irregular verb tenses covered in the lesson.\n",
    "   - Assign homework that involves creating sentences using both past and future tense forms of irregular verbs.\n",
    "\n",
    "Practice Examples:\n",
    "1. Write sentences using both the present and past forms of irregular verbs, such as \"I am a worker\" and \"I was a student.\"\n",
    "2. Create sentences with irregular verb tenses like \"I draw a flower\" and \"I drew a house.\"\n",
    "3. Practice using past tense words like \"Yesterday I made chicken\" and \"Yesterday, I stood in line at the store.\"\n",
    "4. Form sentences with irregular verbs in present and past forms, such as \"I send mail\" and \"I sent a letter.\"\n",
    "5. Utilize phrases with irregular verbs like \"I see you right now\" and \"I saw my mom yesterday.\"\n",
    "\n",
    "Feel free to incorporate these practice examples into your lesson plan to enhance student understanding of irregular verb tenses.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
